/* ==================== */
/* FILE: ./APIServicesGuide.md
/* ==================== */

# API Services Guide

## Available Services

### Anthropic (Claude)
```
ANTHROPIC_API_KEY=your_key
```
- Best for: Complex reasoning, code understanding, step-by-step analysis
- Strengths: Document analysis, consistent structured output, careful validation
- Use when: Extracting detailed EPD data, validating calculations, generating documentation
- Models: Claude 3.5 Sonnet (balanced and currently best), Claude 3: Opus (largest) and Haiku (fastest)

### OpenAI (GPT and o1)
```
OPENAI_API_KEY=your_key
```

**GPT Models**
- Best for: Quick data processing, format conversion, summarization
- Strengths: Fast responses, good with standard formats, broad knowledge
- Use when: Rapid prototyping, simple data transformations, unit conversions
- Models: GPT-4 (most capable), GPT-3.5 (faster, cost-effective)

**o1 Models** (New Reasoning Series)
- o1-preview
  - Best for: Complex scientific analysis, advanced math, multi-step reasoning
  - Strengths: Thorough problem-solving, high accuracy in technical tasks
  - Use when: Complex LCA calculations, methodology validation, technical documentation
  - Rate limit: 50 queries/week
  
- o1-mini
  - Best for: Coding tasks and logical reasoning
  - Strengths: Cost-effective, efficient for programming
  - Use when: Developing data processing scripts, optimization tasks
  - Rate limit: 50 queries/day
  - Cost: 80% cheaper than o1-preview

### Perplexity
```
PERPLEXITY_KEY=your_key
```
- Best for: Research, fact checking, current data verification
- Strengths: Up-to-date information, citations, research synthesis
- Use when: Verifying EPD standards, checking methodologies, finding references, literature search
- Models: pplx-7b-online (fast), pplx-70b-online (more capable)

## Alternative Options
- **Local Models**: Consider installing [Ollama](https://ollama.com/) to load and run Llama 3.2 for fully local processing if your hardware supports the configurations
- **Other Services**: Google Gemini family offers competitive capabilities with exceptionally large context windows
- **Open Models**: Hugging Face and other Model Zoos provide self-hostable options

## Suggested Usage Strategy
1. Initial extraction and/or complex reasoning: Claude 3.5 Sonnet or o1-preview (thorough, structured)
2. Quick validations: GPT or Haiku (fast turnaround)
3. Standards/references: Perplexity (current information)
4. Code development: o1-mini (cost-effective) combineed with Claude 3.5 Sonnet or o1-preview for more complex

## Cost Optimisation
- Prototype with GPT-3.5/Haiku
- Use Chlaude 3.5 Sonnet/o1-preview for final validation
- Cache responses for repeated queries
- Consider local models for high-volume tasks

/* ==================== */
/* FILE: ./CONTRIBUTING.md
/* ==================== */

## Setup Instructions
1. Clone repository
2. Create virtual environment
3. Install requirements
4. Copy .env.template to .env

## Pull Request Process
1. Create team branch
2. Copy team template
3. Complete exercises

/* ==================== */
/* FILE: ./LICENSE
/* ==================== */

MIT License

Copyright (c) 2024 BrightWolves

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


/* ==================== */
/* FILE: ./README.md
/* ==================== */

# brightwolves-lca-ai-workshop

Workshop materials for accelerating Life Cycle Analysis (LCA) using AI/LLM tools. Created by BrightWolves for VTK/Emergent students.

## Overview

This workshop demonstrates how Large Language Models (LLMs) can enhance Life Cycle Assessment (LCA) efficiency and accuracy. Through hands-on exercises, participants will explore using AI to accelerate:

- Data extraction from technical documentation
- Nomenclature harmonisation across datasets
- Missing data inference
- Impact classification automation
- Documentation standardisation

## Prerequisites

- Basic Python programming knowledge
- Understanding of [LCA concepts](https://www.brightwolves.com/post/life-cycle-assessment-lcas-a-powerful-tool-for-extended-producer-responsibility-epr-compliance)
- Basic understanding of [Git](https://git-scm.com/docs/user-manual) and access to [GitHub](https://github.com/) (ideally with your own account)
- API access (instructions provided)

## Quick Start

1. **Prerequisites**
   ```bash
   # Install [uv package manager](https://docs.astral.sh/uv/)
   curl -LsSf https://astral.sh/uv/install.sh | sh  # Unix/macOS
   powershell -c "irm https://astral.sh/uv/install.ps1 | iex"  # Windows
   ```

2. **Clone & Setup**
   ```bash
   # Fork repo via GitHub interface first, then:
   git clone https://github.com/your-username/brightwolves-lca-ai-workshop.git
   cd brightwolves-lca-ai-workshop
   
   # Add upstream remote
   git remote add upstream https://github.com/brightwolves/brightwolves-lca-ai-workshop.git
   ```

3. **Environment Setup**
   ```bash
   # Create and activate environment
   uv venv
   source .venv/bin/activate  # Windows: .venv\Scripts\activate
   
   # Install core dependencies
   uv pip install -r requirements.txt
   
   # Optional: Install development tools
   uv pip install -r requirements-dev.txt  # Adds testing, linting, docs tools
   
   # Setup environment variables
   cp .env.template .env
   # Edit .env with your API keys
   ```

   The dependencies are split into two files:
   - `requirements.txt`: Core packages needed for the workshop exercises
   - `requirements-dev.txt`: Additional tools for testing, code quality, and documentation

   We recommend installing both during the workshop to take advantage of:
   - Code formatting (`black`)
   - Type checking (`mypy`)
   - Testing tools (`pytest`)
   - Documentation generation (`mkdocs`)

4. **Create Team Environment**
   ```bash
   # Create feature branch
   git checkout -b feature/team_XX_setup  # Replace XX with team number
   
   # Copy template
   cp -r solutions/team_template solutions/team_XX
   
   # Commit setup
   git add solutions/team_XX
   git commit -m "team_XX: Initial setup"
   git push origin feature/team_XX_setup
   ```

5. **Create Pull Request**
   - Go to GitHub repository
   - Create PR from your feature branch to main
   - Fill in PR template
   - Request review

## Development Flow

1. **Start New Exercise**
   ```bash
   # Update main
   git checkout main
   git pull upstream main
   
   # Create feature branch
   git checkout -b feature/team_XX_exercise_YY
   ```

2. **Work on Exercise**
   - Code in your team's directory
   - Commit frequently
   - Push to share with team
   - Run tests locally

3. **Submit Solution**
   - Create PR via GitHub
   - Follow PR template
   - Request review
   - Address feedback

## Common Issues

1. **Environment**
   ```bash
   # If uv fails, fallback to pip:
   python -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

2. **Git Issues**
   ```bash
   # Reset local changes
   git checkout -- .
   
   # Update from upstream
   git fetch upstream
   git reset --hard upstream/main
   ```

## Workshop Structure

### 1. Introduction (20 min)
- LCA fundamentals
- AI/LLM integration opportunities
- Tool overview

### 2. Hands-on Exercise (80 min)
Teams work on mineral wool insulation case study:
- Exercise 1 (20 min): Data extraction
- Exercise 2 (25 min): Processing and analysis
- Exercise 3 (25 min): Results analysis and visualisation

### 3. Results Discussion (20 min)
- Team presentations
- Best practices, implementation considerations, key learnings
- Next steps

## Next Steps
1. Complete environment setup
2. Read exercise documentation
3. Start first exercise

## Repository Structure

```
brightwolves-lca-ai-workshop/
├── exercises/                      # Exercise descriptions and starter code
│   ├── __init__.py                 # Makes exercises a package
│   ├── setup_env.py                # Environment setup helper
│   ├── 01_data_extraction/        
│   │   ├── README.md               # Exercise instructions
│   │   └── starter_notebook.ipynb  # Template notebook to copy
│   ├── 02_data_processing/
│   │   ├── README.md
│   │   └── starter_notebook.ipynb
│   └── 03_analysis/
│       ├── README.md
│       └── starter_notebook.ipynb
│
├── solutions/
│   ├── __init__.py                 # Makes solutions a package
│   ├── team_template/              # Template for team solutions
│   │   ├── __init__.py           
│   │   ├── src/                    # Team's source code
│   │   │   ├── __init__.py
│   │   │   ├── extraction.py
│   │   │   ├── processing.py
│   │   │   └── visualization.py
│   │   ├── tests/                  # Team's tests
│   │   │   ├── __init__.py
│   │   │   └── test_extraction.py
│   │   └── notebooks/              # Team's completed exercise notebooks
│   │       ├── 01_data_extraction.ipynb  
│   │       ├── 02_data_processing.ipynb
│   └──     └── 03_analysis.ipynb
│
├── data/                          # Shared data resources
│   └── reference_results/
│
└── setup_env.py                   # Root environment setup (optional)
```

## Development Workflow

We follow trunk-based development with the `main` branch as our trunk. All changes are made through short-lived feature branches and merged via Pull Requests.

### Initial Setup

1. **Fork and Clone**
   ```bash
   # Fork via GitHub interface, then:
   git clone https://github.com/your-username/brightwolves-lca-ai-workshop.git
   cd brightwolves-lca-ai-workshop
   ```

2. **Set Up Remote**
   ```bash
   git remote add upstream https://github.com/brightwolves/brightwolves-lca-ai-workshop.git
   ```

### Team Workflow

1. **Start Fresh**
   ```bash
   # Get latest changes
   git checkout main
   git pull upstream main
   
   # Create short-lived feature branch
   git checkout -b feature/team_XX_exercise_YY
   ```

2. **Copy Template** (first time only)
   ```bash
   cp -r solutions/team_template solutions/team_XX
   ```

3. **Regular Commits**
   ```bash
   # Stage and commit changes
   git add solutions/team_XX
   git commit -m "team_XX: Brief description of changes"
   
   # Push to your fork
   git push origin feature/team_XX_exercise_YY
   ```

4. **Create Pull Request**
   - Create PR via GitHub interface
   - Use the PR template
   - Target the `main` branch
   - Include:
     - Team members
     - Exercise description
     - Approach summary
     - Test results

5. **Review Process**
   - PR needs one approval
   - All checks must pass
   - Keep changes focused and small
   - Address review comments promptly

6. **After Merge**
   ```bash
   # Delete local feature branch
   git checkout main
   git branch -D feature/team_XX_exercise_YY
   
   # Update local main
   git pull upstream main
   ```

### Branch Naming Convention

- Format: `feature/team_XX_exercise_YY`
- Example: `feature/team_01_data_extraction`

### Commit Message Format

```
team_XX: Brief description of change

- Detailed point 1
- Detailed point 2
```

### PR Title Format

```
team_XX: Exercise YY - Brief Description
```

1. **Create Team Branch**
   ```bash
   git checkout -b [team_XX]
   ```

2. **Copy Template**
   ```bash
   cp -r solutions/team_template_solutions/[team_XX]
   ```

3. **Submit Solution**
   ```bash
   git add solutions/[team_XX]
   git commit -m "Team XX: Solution submission"
   git push origin [team_XX]
   ```

4. **Create Pull Request**
   - Use provided template
   - Include team details
   - Document approach

## Resources
- [ ] TODO
    - [LCA Background]([link-to-resources](https://www.brightwolves.com/post/life-cycle-assessment-unveiling-iso-standards-gfli-in-our-newest-whitepaper-series)) 
    - [API Documentation](link-to-docs)
    - [Example Solutions](link-to-examples)

## Getting Help

- Technical issues: [Open GitHub Issue](https://github.com/BrightWolves/brightwolves-lca-ai-workshop/issues)
- Workshop questions: [Use Discussion board](https://github.com/BrightWolves/brightwolves-lca-ai-workshop/discussions)
- Emergency: Contact workshop lead

## License

This project is licensed under the MIT License with Attribution - see the [LICENSE](LICENSE) file for details.

## Citation

If you use these materials in your work, please cite:

```bibtex
@software{brightwolves_lca_ai_2024,
  author = {BrightWolves},
  title = {LCA AI Workshop},
  year = {2024},
  url = {https://github.com/brightwolves/brightwolves-lca-ai-workshop}
}
```

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed contribution guidelines.

## Acknowledgments

- [Emergent Leuven](https://emergentleuven.be/) and [Vlaamse Technische Kring (VTK)](https://vtk.be/) for collaboration
- Workshop participants for feedback
- Open-source tools and libraries used

## Contact

For questions about this workshop or BrightWolves' LCA services:
- Email: [info@brightwolves.eu](mailto:info@brightwolves.eu)
- Website: [brightwolves.com](https://brightwolves.com)

/* ==================== */
/* FILE: ./data/reference_results/validation_dataset.csv
/* ==================== */

TODO

/* ==================== */
/* FILE: ./exercises/__init__.py
/* ==================== */

"""Exercises package initialization."""

/* ==================== */
/* FILE: ./exercises/setup_env.py
/* ==================== */

"""Environment setup helper for workshop notebooks.

This module helps set up the Python path for workshop notebooks to ensure
proper importing of team solution packages and access to workshop data.
"""
import os
import sys
from pathlib import Path
from typing import Optional, Dict, Any
import json


def find_repo_root(start_path: Optional[Path] = None) -> Path:
    """Find the repository root by looking for key directories.
    
    Args:
        start_path: Path to start searching from. Defaults to current directory.
    
    Returns:
        Path to repository root
        
    Raises:
        RuntimeError: If repository root cannot be found
    """
    if start_path is None:
        start_path = Path().absolute()
        
    current = start_path
    while current != current.parent:
        # Look for key directories that indicate repo root
        if all((current / d).exists() for d in ['exercises', 'solutions', 'data']):
            return current
        current = current.parent
        
    raise RuntimeError(
        f"Could not find repository root from {start_path}. "
        "Make sure you're running this from within the workshop repository."
    )


def verify_data_files() -> Dict[str, Path]:
    """Verify existence of required data files and return their paths.
    
    Returns:
        Dictionary mapping file descriptions to their paths
        
    Raises:
        FileNotFoundError: If required data files are missing
    """
    repo_root = find_repo_root()
    data_dir = repo_root / "data" / "reference_results"
    
    required_files = {
        'sample_epd': data_dir / 'sample_epd.pdf',
        'reference_calculations': data_dir / 'reference_calculations.csv',
        'validation_dataset': data_dir / 'validation_dataset.csv'
    }
    
    missing_files = [
        desc for desc, path in required_files.items() 
        if not path.exists()
    ]
    
    if missing_files:
        raise FileNotFoundError(
            f"Missing required data files: {', '.join(missing_files)}"
        )
        
    return required_files


def verify_team_solution(team_name: str = "team_template") -> None:
    """Verify team solution directory structure is correct.
    
    Args:
        team_name: Name of team directory to verify
        
    Raises:
        RuntimeError: If team directory structure is invalid
    """
    repo_root = find_repo_root()
    team_dir = repo_root / "solutions" / team_name
    
    required_dirs = [
        team_dir / "src",
        team_dir / "notebooks",
        team_dir / "tests",
        team_dir / "prompts"
    ]
    
    missing_dirs = [
        d for d in required_dirs 
        if not d.exists()
    ]
    
    if missing_dirs:
        raise RuntimeError(
            f"Invalid team directory structure. Missing: {', '.join(str(d) for d in missing_dirs)}"
        )


def setup_notebook_env() -> Dict[str, Any]:
    """Set up Python path and verify workshop environment.
    
    Returns:
        Dictionary containing environment information
        
    Raises:
        RuntimeError: If environment setup fails
    """
    try:
        # Find repo root
        repo_root = find_repo_root()
        
        # Add paths to Python path
        paths_to_add = [
            str(repo_root),
            str(repo_root / "solutions")
        ]
        
        for path in paths_to_add:
            if path not in sys.path:
                sys.path.append(path)
                
        # Verify environment
        data_files = verify_data_files()
        verify_team_solution()
        
        # Try imports
        import team_template.src.extraction
        import team_template.src.processing
        
        env_info = {
            'repo_root': repo_root,
            'data_files': data_files,
            'python_paths': paths_to_add
        }
        
        print("Environment setup complete! ✨")
        print("\nPython path additions:")
        for path in paths_to_add:
            print(f"  - {path}")
        print("\nVerified data files:")
        for desc, path in data_files.items():
            print(f"  - {desc}: {path}")
            
        return env_info
        
    except Exception as e:
        print(f"❌ Environment setup failed: {str(e)}")
        print("\nPlease make sure:")
        print("1. You're running this notebook from within the workshop repository")
        print("2. All required data files are present")
        print("3. Your team solution directory is properly structured")
        raise


if __name__ == "__main__":
    setup_notebook_env()

/* ==================== */
/* FILE: ./pyproject.toml
/* ==================== */

[project]
name = "brightwolves-lca-ai-workshop"
version = "0.1.0"
description = "Workshop materials for accelerating Life Cycle Analysis using AI/LLM tools"
authors = [
    {name = "BrightWolves", email = "contact@brightwolves.eu"},
]
requires-python = ">=3.10"

[tool.black]
line-length = 100
target-version = ['py310']

[tool.ruff]
select = ["E", "F", "B", "I"]
line-length = 100
target-version = "py310"
fix = true

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
check_untyped_defs = true

[tool.pytest.ini_options]
minversion = "8.0"
addopts = "-ra -q --cov=src"
testpaths = ["tests"]

/* ==================== */
/* FILE: ./reposity_strucutre_guide.md
/* ==================== */

# Repository Guide

## Prompt Engineering and Management

### Structure
- `prompts/` directory in team folders for version control of prompts
- Each prompt type has its own file (extraction.md, validation.md, etc.)
- Prompts are treated like code: reviewed, tested, and versioned
- A good example is given here: [solutions/team_template/prompts/extraction/example_extraction.md](./solutions/team_template/prompts/extraction/example_extraction.md)

### Prompt Types
```
prompts/
├── extraction/             # EPD data extraction
│   ├── base_prompt.md     # Core extraction strategy
│   └── refinements/       # Specific field extractors
├── validation/            # Data validation
│   ├── unit_checks.md    
│   └── completeness.md
└── reporting/             # Documentation generation
    └── summary.md
```

### Best Practices
1. **Version Control**
   - Track prompt changes in git
   - Document why prompts were modified
   - Keep prompt history for comparison

2. **Testing**
   ```python
   def test_extraction_prompt():
       """Test prompt against known EPD data"""
       result = run_prompt('prompts/extraction/base_prompt.md', test_epd)
       assert_extracted_fields(result)
   ```

3. **Documentation**
   - Note which model version used
   - Track performance metrics
   - Document edge cases

### Example Prompt Structure
```markdown
# Data Extraction Prompt

## Context
Extracting data from Environmental Product Declaration (EPD)

## Task Description
Extract specific environmental impact values and units

## Required Format
JSON with fields:
- impact_category
- value
- unit
- confidence

## Example Input/Output
[Examples of successful extractions]

## Additional Instructions
- Handle unit conversions
- Note data quality
- Flag uncertainties
```

## Project Organization

### Notebooks vs. Source Code

The project uses both Jupyter notebooks (`.ipynb`) and Python source files (`.py`) for different purposes:

**Notebooks (`notebooks/`)**: Interactive development and exploration
- `01_environment_setup.ipynb`: Verify tool installation and API access
- `02_data_extraction.ipynb`: Explore EPD document parsing techniques
- `03_data_processing.ipynb`: Develop data cleaning and analysis approaches
- `04_visualization.ipynb`: Create and refine visualizations

**Source Code (`src/`)**: Reusable, tested implementation
- `extraction.py`: Clean functions for extracting data from EPDs
- `processing.py`: Data transformation and LCI calculations
- `validation.py`: Data quality checks and result validation
- `visualization.py`: Plotting functions for standard visualizations

**Why This Split?**
- Notebooks: Experimentation, visualization, learning
- Source: Clean, documented, tested code
- Best practice: Develop in notebooks, refactor to source
- Enables proper testing and version control

## Code Quality Tools Explained

- **Formatting (black)**: Ensures consistent code style across all teams
- **Linting (ruff)**: Catches common mistakes and enforces best practices
- **Type Checking (mypy)**: Prevents type-related bugs before they occur
- **Testing (pytest)**: Verifies code works as intended
- **Coverage**: Measures test completeness by tracking executed code

## Key Files

### Configuration Files
- `pyproject.toml`: Project metadata and tool configurations (black, ruff, mypy, pytest)
   + This is a standard Python project configuration file introduced with PEP 518, used to specify project metadata and tool configurations. In this case, it defines:

      1. Project metadata (name, version, description, authors)
      2. Python version requirement (>=3.10)
      3. Tool configurations for:
         - black (code formatter)
         - ruff (linter)
         - mypy (type checker)
         - pytest (testing framework)

   + It's placed in the root directory of the project and is used by modern Python tools like pip, build systems, and development tools to understand how to handle the project.
- `.env.template`: Template for API keys and configuration
- `.gitignore`: Specifies which files Git should ignore

### Dependency Management
- `requirements.txt`: Core dependencies for workshop exercises
- `requirements-dev.txt`: Additional tools for development
  - Documentation generators
  - Testing utilities
  - Code quality tools

### GitHub Templates
- `.github/PULL_REQUEST_TEMPLATE/`: PR submission guidelines
- `.github/ISSUE_TEMPLATE/`: Bug report and feature request templates

### Team Workspace
```
solutions/team_XX/
├── src/              # Source code
├── tests/            # Test cases
├── notebooks/        # Jupyter notebooks
└── README.md         # Team documentation
```

## Development Tools

### Code Quality
```bash
# Format code - Automatically fixes code style to match project standards
black solutions/team_XX

# Lint code - Checks for potential errors, style violations, and code smells
ruff check solutions/team_XX

# Type checking - Verifies correct usage of data types to prevent runtime errors
mypy solutions/team_XX
```

### Testing
```bash
# Run tests - Executes test suite to verify code functionality
pytest solutions/team_XX/tests

# With coverage - Shows which lines of code are executed during tests
# Helps identify untested code paths
pytest --cov=solutions/team_XX
```

### Documentation
```bash
# Generate docs
mkdocs serve
```

## Dependencies

### Core (`requirements.txt`)
- Data processing: pandas, numpy
- Visualization: matplotlib, seaborn
- PDF handling: pdfplumber, PyPDF2
- API integration: requests, anthropic, openai

### Development (`requirements-dev.txt`)
- Testing: pytest, pytest-cov
- Formatting: black, ruff
- Type checking: mypy
- Documentation: mkdocs

## Environment Variables
Required in `.env`:
```
ANTHROPIC_API_KEY=your_key
OPENAI_API_KEY=your_key
PERPLEXITY_KEY=your_key
```

## Best Practices
- Keep notebooks clean and documented
- Run code quality tools before commits
- Update team README with approach details
- Include tests for core functionality

/* ==================== */
/* FILE: ./requirements-dev.txt
/* ==================== */

-r requirements.txt

# Additional development tools
ipython>=8.22.1
jupyterlab>=4.1.0
pre-commit>=3.6.0
pytest-watch>=4.2.0
rich>=13.7.0

# Documentation
mkdocs>=1.5.0
mkdocs-material>=9.5.0
mkdocstrings>=0.24.0


/* ==================== */
/* FILE: ./requirements.txt
/* ==================== */

# Core dependencies
jupyter
pandas
numpy
matplotlib
seaborn

# PDF processing
pdfplumber
PyPDF2

# API and environment
requests
python-dotenv
anthropic
openai

# Testing
pytest
pytest-cov

/* ==================== */
/* FILE: ./solutions/team_template/README.md
/* ==================== */

# Team [XX] Solution

## Team Members
- Member 1
- Member 2
- Member 3

## Our Approach
[Describe your approach here]

## Results
[Present your results here]

## Key Learnings
[Share your learnings here]

/* ==================== */
/* FILE: ./solutions/team_template/prompts/extraction/base_prompt.md
/* ==================== */

# EPD Data Extraction

## Context
Extracting environmental impact data from EPD document.

## Task
Extract standardized impact categories and values.

## Input Format
EPD text with tables and sections.

## Output Format
```json
{
  "impact_categories": [],
  "metadata": {}
}
```

## Validation Rules
- Values must be numerical
- Units must be present
- Confidence levels required


/* ==================== */
/* FILE: ./solutions/team_template/prompts/extraction/example_extraction.md
/* ==================== */

# EPD Data Extraction Prompt

## Context
You are analyzing an Environmental Product Declaration (EPD) document to extract specific environmental impact values. The document follows EN 15804 standards for construction products.

## Task
Extract all environmental impact categories and their values from the EPD text, maintaining data quality through validation checks.

## Input Format
EPD text will be provided as plain text, potentially including tables and sections.

## Required Output Format
```json
{
  "impact_categories": [
    {
      "name": "Global Warming Potential",
      "value": 12.3,
      "unit": "kg CO2 eq.",
      "confidence": "high",
      "data_quality_notes": "Directly extracted from table"
    }
  ],
  "metadata": {
    "extraction_timestamp": "2024-03-22T10:00:00Z",
    "epd_reference": "EPD-123",
    "validation_checks_passed": true
  }
}
```

## Instructions
1. Identify environmental impact tables
2. Extract values with units
3. Validate data coherence
4. Note any assumptions or uncertainties
5. Maintain source traceability

## Validation Requirements
- All values must have units
- Units must be standardized
- Values must be numerical
- Confidence levels must be justified

## Example
Input:
```
Global Warming Potential (GWP100): 12.3 kg CO2 eq.
Acidification Potential: 0.08 kg SO2 eq.
```

Output:
```json
{
  "impact_categories": [
    {
      "name": "Global Warming Potential",
      "value": 12.3,
      "unit": "kg CO2 eq.",
      "confidence": "high",
      "data_quality_notes": "Direct value from text"
    },
    {
      "name": "Acidification Potential",
      "value": 0.08,
      "unit": "kg SO2 eq.",
      "confidence": "high",
      "data_quality_notes": "Direct value from text"
    }
  ],
  "metadata": {
    "extraction_timestamp": "2024-03-22T10:00:00Z",
    "epd_reference": "Example",
    "validation_checks_passed": true
  }
}
```

/* ==================== */
/* FILE: ./solutions/team_template/prompts/extraction/refinements/field_extractor.md
/* ==================== */

# Field Extraction

## Target Field
[Specify field]

## Extraction Rules
- [Rule 1]
- [Rule 2]

## Examples
Input: [example]
Output: [expected]


/* ==================== */
/* FILE: ./solutions/team_template/prompts/reporting/summary.md
/* ==================== */

# Report Generation

## Format
- Executive summary
- Methodology
- Results
- Data quality assessment

## Requirements
- Clear data presentation
- Uncertainty documentation
- Methodology transparency


/* ==================== */
/* FILE: ./solutions/team_template/prompts/validation/completeness.md
/* ==================== */

# Completeness Check

## Required Fields
- Impact categories
- Values
- Units
- Confidence levels

## Validation Steps
1. Check all required fields
2. Verify data quality
3. Flag missing information


/* ==================== */
/* FILE: ./solutions/team_template/prompts/validation/unit_checks.md
/* ==================== */

# Unit Validation

## Task
Verify unit consistency and conversions.

## Validation Rules
- Standard unit format
- Valid conversion factors
- Dimensional analysis


/* ==================== */
/* FILE: ./solutions/team_template/src/extraction.py
/* ==================== */

class EPDExtractor:
    """Base class for EPD data extraction with pre-implemented helpers."""
    
    def __init__(self, model_name: str = "claude-3-5-sonnet-latest"): # Currently latest is claude-3-5-sonnet-20241022 at time of writing
        self.model = model_name
        self.extraction_prompt = load_prompt("extraction/base_prompt.md")
    
    def extract_from_pdf(self, pdf_path: str) -> dict:
        """Extract structured data from EPD PDF."""
        text = self._extract_text(pdf_path)
        tables = self._extract_tables(pdf_path)
        return self._process_content(text, tables)
    
    def validate_extraction(self, data: dict) -> tuple[bool, list[str]]:
        """Validate extracted data against known patterns."""
        # Pre-implemented validation logic
        pass



/* ==================== */
/* FILE: ./solutions/team_template/src/processing.py
/* ==================== */

class LCAProcessor:
    """Core LCA calculations and data processing."""
    
    def __init__(self):
        self.unit_conversions = load_conversions()
        self.impact_categories = load_categories()
    
    def standardize_units(self, value: float, 
                         from_unit: str, 
                         to_unit: str) -> float:
        """Convert between common LCA units."""
        pass
    
    def calculate_impacts(self, 
                         inventory_data: dict,
                         impact_factors: dict) -> dict:
        """Calculate environmental impacts from inventory."""
        pass

/* ==================== */
/* FILE: ./solutions/team_template/src/visualization.py
/* ==================== */

class LCAVisualizer:
    """Pre-built visualization templates for common LCA charts."""
    
    @staticmethod
    def plot_impact_comparison(data: dict, 
                             categories: list[str]) -> None:
        """Generate standard impact category comparison chart."""
        pass
    
    @staticmethod
    def plot_contribution_analysis(data: dict,
                                 category: str) -> None:
        """Create contribution analysis visualization."""
        pass

/* ==================== */
/* FILE: ./solutions/team_template/tests/test_extraction.py
/* ==================== */

def test_epd_extraction():
    """Test extraction of key fields from sample EPD."""
    extractor = EPDExtractor()
    result = extractor.extract_from_pdf("data/reference_results/sample_epd.pdf")
    assert "impact_categories" in result
    # Add more specific test cases

/* ==================== */
/* FILE: ./solutions/team_template/tests/test_processing.py
/* ==================== */

def test_unit_conversion():
    """Test conversion between common units."""
    processor = LCAProcessor()
    result = processor.standardize_units(1000, "g", "kg")
    assert result == 1.0

/* ==================== */
/* FILE: ./solutions/team_template/__init__.py
/* ==================== */

"""Team template package initialization."""

/* ==================== */
/* FILE: ./solutions/__init__.py
/* ==================== */

"""Solutions package initialization."""

/* ==================== */
/* FILE: ./combined_files.txt
/* ==================== */

